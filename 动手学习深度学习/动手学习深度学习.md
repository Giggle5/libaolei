__第二章__

要学习深度学习，首先需要先掌握一些基本技能。 所有机器学习方法都涉及从数据中提取信息。 因此，我们先学习一些关于数据的实用技能，包括存储、操作和预处理数据。
机器学习通常需要处理大型数据集。 我们可以将某些数据集视为一个表，其中表的行对应样本，列对应属性。 线性代数为人们提供了一些用来处理表格数据的方法。 我们不会太深究细节，而是将重点放在矩阵运算的基本原理及其实现上。
深度学习是关于优化的学习。 对于一个带有参数的模型，我们想要找到其中能拟合数据的最好模型。 在算法的每个步骤中，决定以何种方式调整参数需要一点微积分知识。 本章将简要介绍这些知识。 幸运的是，autograd包会自动计算微分，本章也将介绍它。
机器学习还涉及如何做出预测：给定观察到的信息，某些未知属性可能的值是多少？ 要在不确定的情况下进行严格的推断，我们需要借用概率语言。
主要作为预备知识熟悉python的语法和基本技能，这里选择我们tensorflow，对第二章的基本代码运行一遍，了解其功能，导入所需的包 tensorflow，d2l等。注意于python版本匹配
Python 3.12 变化：Python 3.12 移除了 pkgutil.ImpImporter（官方说明）

setuptools 兼容性问题：旧版 setuptools (<68.0) 使用了这个被移除的特性

常见影响包：d2l、tensorflow、pytorch 等依赖复杂构建流程的包
!(https://github.com/Giggle5/libaolei/blob/main/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image/QQ%E6%88%AA%E5%9B%BE20250619225928.png)
熟悉python语法了解线性代数，微积分，概率等使用加深机器学习的理解，学会数据预处理基本流程。

__第三章__

__线性神经网络__
在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。
回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。
在机器学习领域中的大多数任务通常都与预测（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的预测都是回归问题。 在后面的章节中，我们将介绍分类问题。分类问题的目标是预测数据属于一组类别中的哪一个。
 __softmax运算__
现在我们将优化参数以最大化观测数据的概率。 为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。

我们希望模型的输出
可以视为属于类的概率， 然后选择具有最大输出值的类别作为我们的预测。然而我们能否将未规范化的预测直接视作我们感兴趣的输出呢？ 答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。 这些违反了所说的概率基本公理。
要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做校准（calibration）。
尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。
__第四章多层感知机__

在 3节中， 我们介绍了softmax回归（ 3.4节）， 然后我们从零开始实现了softmax回归（ 3.6节）， 接着使用高级API实现了算法（ 3.7节）， 并训练分类器从低分辨率图像中识别10类服装。 在这个过程中，我们学习了如何处理数据，如何将输出转换为有效的概率分布， 并应用适当的损失函数，根据模型参数最小化损失。 我们已经在简单的线性模型背景下掌握了这些知识， 现在我们可以开始对深度神经网络的探索，这也是本书主要涉及的一类模型。
前向传播，反向传播的定义
前向传播、反向传播和计算图

我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过前向传播（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。
梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨反向传播的细节。 首先，我们将重点放在带权重衰减（L2正则化）的单隐藏层多层感知机上。
前向传播
前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。
前向传播计算图
绘制计算图有助于我们可视化计算中操作符和变量的依赖关系。 :numref:fig_forward 是与上述简单网络相对应的计算图， 其中正方形表示变量，圆圈表示操作符。 左下角表示输入，右上角表示输出。 注意显示数据流的箭头方向主要是向右和向上的。
反向传播
反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 假设我们有函数Y=f(X)和Z=g(Y)， 其中输入和输出X,Y,Z是任意形状的张量。 利用链式法则，我们可以计算Z关于X的导数
__第五章 深度学习计算__
