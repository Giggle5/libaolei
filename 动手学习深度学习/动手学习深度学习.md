__第二章__

要学习深度学习，首先需要先掌握一些基本技能。 所有机器学习方法都涉及从数据中提取信息。 因此，我们先学习一些关于数据的实用技能，包括存储、操作和预处理数据。
机器学习通常需要处理大型数据集。 我们可以将某些数据集视为一个表，其中表的行对应样本，列对应属性。 线性代数为人们提供了一些用来处理表格数据的方法。 我们不会太深究细节，而是将重点放在矩阵运算的基本原理及其实现上。
深度学习是关于优化的学习。 对于一个带有参数的模型，我们想要找到其中能拟合数据的最好模型。 在算法的每个步骤中，决定以何种方式调整参数需要一点微积分知识。 本章将简要介绍这些知识。 幸运的是，autograd包会自动计算微分，本章也将介绍它。
机器学习还涉及如何做出预测：给定观察到的信息，某些未知属性可能的值是多少？ 要在不确定的情况下进行严格的推断，我们需要借用概率语言。
主要作为预备知识熟悉python的语法和基本技能，这里选择我们pytorch，对第二章的基本代码运行一遍，了解其功能，导入所需的包torch，d2l等。注意于python版本匹配
Python 3.12 变化：Python 3.12 移除了 pkgutil.ImpImporter（官方说明）

setuptools 兼容性问题：旧版 setuptools (<68.0) 使用了这个被移除的特性

常见影响包：d2l、torch 等依赖复杂构建流程的包
主要内容：
数据操作有张量运算，索引切片，广播机制。

数据预处理：将csv文件处理缺失值，转换成张量类型。

线性代数：矩阵运算。张量运算。 部分名词解释 范式 ，标量，降维。

微积分：偏导，梯度，链式法则，自动求导（正向累积，反向传递）。

概率

熟悉python语法了解线性代数，微积分等使用加深机器学习的理解，学会数据预处理基本流程。

__第三章__

__线性神经网络__。

在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。
回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。
在机器学习领域中的大多数任务通常都与预测（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的预测都是回归问题。 在后面的章节中，我们将介绍分类问题。分类问题的目标是预测数据属于一组类别中的哪一个。

__线性回归__

线性模型
损失函数
训练数据
随机梯度下降   梯度：函数值增加最快的方向 批量大小 学习率

线性回归从零开始实现
线性回归的简洁实现

 __softmax回归__

分类问题   正确类的预测值。

softmax和交叉熵

图像分类数据集

softmax回归的从零开始实现
softmax简洁实现


__第四章多层感知机__

在 3节中， 我们介绍了softmax回归（ 3.4节）， 然后我们从零开始实现了softmax回归（ 3.6节）， 接着使用高级API实现了算法（ 3.7节）， 并训练分类器从低分辨率图像中识别10类服装。 在这个过程中，我们学习了如何处理数据，如何将输出转换为有效的概率分布， 并应用适当的损失函数，根据模型参数最小化损失。 我们已经在简单的线性模型背景下掌握了这些知识， 现在我们可以开始对深度神经网络的探索，这也是本书主要涉及的一类模型。  


感知机


多层感知机




 训练误差和泛化误差
为了进一步讨论这一现象，我们需要了解训练误差和泛化误差。 训练误差（training error）是指， 模型在训练数据集上计算得到的误差。 泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。  

当我们比较训练和验证误差时，我们要注意两种常见的情况。 首先，我们要注意这样的情况：训练误差和验证误差都很严重， 但它们之间仅有一点差距。 如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足）， 无法捕获试图学习的模式。 此外，由于我们的训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。
另一方面，当我们的训练误差明显低于验证误差时要小心， 这表明严重的过拟合（overfitting）。 注意，过拟合并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。 最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。
是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小， 这两个点将在下面进行讨论。  

我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过前向传播（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。
梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨反向传播的细节。 首先，我们将重点放在带权重衰减（L2正则化）的单隐藏层多层感知机上。  

前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。  

反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。  


__第五章 深度学习计算__

除了庞大的数据集和强大的硬件，优秀的软件工具在深度学习的快速发展中发挥了不可或缺的作用。从2007年
发布的开创性的Theano库开始，灵活的开源工具使研究人员能够快速开发模型原型，避免了我们使用标准组
件时的重复工作，同时仍然保持了我们进行底层修改的能力。随着时间的推移，深度学习库已经演变成提供
越来越粗糙的抽象。就像半导体设计师从指定晶体管到逻辑电路再到编写代码一样，神经网络研究人员已经
从考虑单个人工神经元的行为转变为从层的角度构思网络，通常在设计架构时考虑的是更粗糙的块（block）。
之前我们已经介绍了一些基本的机器学习概念，并慢慢介绍了功能齐全的深度学习模型。在上一章中，我们
从零开始实现了多层感知机的每个组件，然后展示了如何利用高级API轻松地实现相同的模型。为了易于学
习，我们调用了深度学习库，但是跳过了它们工作的细节。在本章中，我们将深入探索深度学习计算的关键
组件，即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘，以及利用GPU实现显著的
加速。这些知识将使读者从深度学习“基础用户”变为“高级用户”。虽然本章不介绍任何新的模型或数据
集，但后面的高级模型章节在很大程度上依赖于本章的知识。

__第六章 卷积神经网络__
在前面的章节中，我们遇到过图像数据。这种数据的每个样本都由一个二维像素网格组成，每个像素可能是
一个或者多个数值，取决于是黑白还是彩色图像。到目前为止，我们处理这类结构丰富的数据的方式还不够
有效。我们仅仅通过将图像数据展平成一维向量而忽略了每个图像的空间结构信息，再将数据送入一个全连
接的多层感知机中。因为这些网络特征元素的顺序是不变的，因此最优的结果是利用先验知识，即利用相近
像素之间的相互关联性，从图像数据中学习得到有效的模型。
本章介绍的卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的
神经网络。基于卷积神经网络架构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、
目标检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础。
现代卷积神经网络的设计得益于生物学、群论和一系列的补充实验。卷积神经网络需要的参数少于全连接架
构的网络，而且卷积也很容易用GPU并行计算。因此卷积神经网络除了能够高效地采样从而获得精确的模型，
还能够高效地计算。久而久之，从业人员越来越多地使用卷积神经网络。即使在通常使用循环神经网络的一
维序列结构任务上（例如音频、文本和时间序列分析），卷积神经网络也越来越受欢迎。通过对卷积神经网络
一些巧妙的调整，也使它们在图结构数据和推荐系统中发挥作用。
在本章的开始，我们将介绍构成所有卷积网络主干的基本元素。这包括卷积层本身、填充（padding）和步幅
（stride）的基本细节、用于在相邻区域汇聚信息的汇聚层（pooling）、在每一层中多通道（channel）的使用，
以及有关现代卷积网络架构的仔细讨论。在本章的最后，我们将介绍一个完整的、可运行的LeNet模型：这是
第一个成功应用的卷积神经网络，比现代深度学习兴起时间还要早。在下一章中，我们将深入研究一些流行
的、相对较新的卷积神经网络架构的完整实现，这些网络架构涵盖了现代从业者通常使用的大多数经典技术。
